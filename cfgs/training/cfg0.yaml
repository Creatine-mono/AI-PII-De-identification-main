##############################################
# Fine-tuning Encoder Model for Korean NER
##############################################

# Paths
paths:
  data:
    # 파이썬 스크립트에서 직접 경로를 지정하므로 이 부분은 참고용으로 두거나,
    # 스크립트에서 이 값을 읽어오도록 수정할 수 있습니다.
    # 현재 스크립트는 아래 경로를 사용하지 않습니다.
    source_file: mdd-gen/llama3_placeholder_2.3K_v0.jsonl

# Saving models and results
save_results:
  model_results: True # 학습된 모델 가중치를 저장합니다.
  wandb_online: True  # W&B 온라인 로깅을 사용합니다.

# DEBUG [True or False]
debug: False

# Data processing (현재 스크립트에서 사용되지 않음)
data:
  split: False
  ds_factor: Null

# wandb
wandb:
  mode: online

# Model and tokenizer
model:
  name: klue/roberta-large 
  freeze:
    apply: False         
    num_layers: 0         

# Trainer arguments
train_args:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 2
  num_train_epochs: 3
  learning_rate: 2.5E-5
  fp16: True
  gradient_checkpointing: False
  use_reentrant: False
  eval_epoch_fraction: 0.1
  warmup_ratio: 0.1 
  weight_decay: 0.01 
  metric_for_best_model: f1
  greater_is_better: True
  lr_scheduler_type: 'linear'

# Tokenizer
tokenizer:
  add_tokens: Null
  do_lower: False
  use_fast: True
  pad_to_multiple_of: 16
  max_token_length: 256 
  stride: 16

# Adjust class weights
class_weights:
  apply: True       
  approach: mean
  multiplier: 0.25

# Focal Loss
focal_loss:
  apply: False      
  alpha: 0.5
  gamma: 2
